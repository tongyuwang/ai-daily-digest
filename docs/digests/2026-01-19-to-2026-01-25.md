# AI Digest - Jan 19 to Jan 25, 2026

*Weekly roundup of AI bookmarks from X/Twitter*

---

## Key Themes

1. **Agents.md & Agent Configuration as Art** — The community consolidated around how to write great AGENTS.md files, with Bilgin Ibryam's analysis of 2,500+ repositories and Anthropic releasing Claude's full 15,000-word "soul."
2. **OpenClaw & AI Employees** — OpenClaw emerged as a framework for onboarding AI as employees, with IDENTITY.md, USER.md, and SOUL.md creating persistent agent personas.
3. **The "Don't Read Code" Revolution** — Kieran Klaassen's provocative claim that code reviews got *better* when he stopped reading code resonated deeply with the agent-first crowd.
4. **Prompting as a Skill Tree** — Aakash Gupta's "prompting like Excel" framework — always a next level — reframed prompting as a progressive skill rather than a fixed technique.

## Synthesis

A quieter but more philosophical week. The discourse shifted from "what tools to use" to "how to think about human-agent relationships." Bilgin Ibryam analyzed 2,500+ repositories to distill what makes a great agents.md, while Anthropic themselves published Claude's 15,000-word constitution — the "soul" behind the model. OpenClaw became a conversation starter about treating AI agents as employees with identity documents (IDENTITY.md, USER.md, SOUL.md). Kieran Klaassen's revelation that he stopped reading code in reviews — 27 files, 1,000+ lines, no bugs — was either terrifying or liberating depending on your perspective. Meanwhile, Eleanor Berger pushed agent-browser.dev as a lighter alternative to Playwright, Miles Deutscher shared Anthropic's 50 best practices, and NVIDIA released PersonaPlex-7B for full-duplex conversations.

---

## Bookmarks

### Agent Configuration & Identity

- **@Bilgin Ibryam** — How to write a great agents.md — lessons distilled from 2,500+ repositories.
  → [x.com/BilginIbryam/status/2013626318391288151](https://x.com/BilginIbryam/status/2013626318391288151)

- **@Aakash Gupta** — Anthropic released Claude's soul/Constitution — a 15,000-word document defining the model's values and behavior.
  → [x.com/AakashGupta/status/2014206254143660060](https://x.com/AakashGupta/status/2014206254143660060)

- **@Corey Ganim** — How to onboard a new AI employee using OpenClaw — IDENTITY.md, USER.md, SOUL.md.
  → [x.com/CoreyGanim/status/2015415731622486254](https://x.com/CoreyGanim/status/2015415731622486254)

- **@Miles Deutscher** — Anthropic Claude Code Best Practices Doc — 50 practical tips for configuration.
  → [x.com/MilesDeutscher/status/2014767612727865837](https://x.com/MilesDeutscher/status/2014767612727865837)

### The "Don't Read Code" Movement

- **@Kieran Klaassen** — "Stopped reading code — code reviews got better. 27 files, 1,000+ lines, no bugs." Trust the agent.
  → [x.com/KieranKlaassen/status/2014803506985918564](https://x.com/KieranKlaassen/status/2014803506985918564)

### Prompting & Interaction Patterns

- **@Aakash Gupta** — Prompting like Excel: always a next level. Act as, engineer context, few-shot — a skill tree framework.
  → [x.com/AakashGupta/status/2013338575052423277](https://x.com/AakashGupta/status/2013338575052423277)

- **@Pedro Domingos** — "Internet cacophony lives inside LLMs" — the philosophical implications of training data.
  → [x.com/PedroDomingos/status/2013763776810504217](https://x.com/PedroDomingos/status/2013763776810504217)

### New Tools & Models

- **@Eleanor Berger** — agent-browser.dev + SKILL instead of Playwright — fast, light, context-friendly browser automation for agents.
  → [x.com/EleanorBerger/status/2013553716549558451](https://x.com/EleanorBerger/status/2013553716549558451)

- **@Charly Wargnier** — NVIDIA PersonaPlex-7B: full-duplex conversational model, open source MIT licensed.
  → [x.com/CharlyWargnier/status/2013892316105417082](https://x.com/CharlyWargnier/status/2013892316105417082)

- **@Bin Liu** — HeyGen + Remotion tutorial — combining AI video generation with programmatic video editing.
  → [x.com/BinLiu/status/2015584909910577477](https://x.com/BinLiu/status/2015584909910577477)
