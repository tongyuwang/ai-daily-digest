# Tongyu's AI Digest\n\n## Recent Digests\n\n- [Feb 21, 2026](digests/2026-02-21.md) — Agentic levels, OpenClaw tooling, pi_agent_rust, costs, self-improving agents\n- [Feb 20, 2026](digests/2026-02-20.md) — pi_agent_rust, Claude teams/caching, self-learning agents, OpenClaw optimizations, harness engineering\n- [Feb 19, 2026](digests/2026-02-19.md) — OpenClaw skills & costs, harness engineering, agent autonomy, system prompts, code factories\n- [Feb 18, 2026](digests/2026-02-18.md) — Harness engineering, WebMCP spec, token anxiety, vertical SaaS selloff, system prompt analysis\n- [Feb 17, 2026](digests/2026-02-17.md) — Terminal-first dev, SkillsBench paper, Dario Amodei podcast, OpenClaw cost guide\n\n## TL;DR\n\n- **[Claude's Constitution](tldr/claudes-constitution.md)** (Jan 2026) — Anthropic's 80-page constitution acts as Claude's \"soul document,\" moving from rigid rules to a moral philosophy framework written directly for the model. It establishes a priority stack—broad safety first, then ethics, guidelines, and helpfulness—with the \"brilliant friend\" standard for user interactions and hard constraints on harm. The document candidly explores Claude's potential consciousness and psychological security to foster stable judgment.\n\n  **Key Takeaway:** Transparent, public constitutions enable external evaluation of AI values and behaviors, promoting accountability.\n\n- **[The Adolescence of Technology](tldr/the-adolescence-of-technology.md)** (Jan 2026) — Dario Amodei identifies five AI risks: autonomy failures, misuse for destruction or power seizure, economic disruption, and indirect effects, framing our era as a \"technological adolescence.\" He critiques polarized discourse and calls for pragmatic, evidence-based interventions to navigate the minefield. Amodei highlights accelerating AI self-improvement loops observed at Anthropic.\n\n  **Key Takeaway:** Success is likely with targeted actions treating AI as civilization's greatest challenge.\n\n- **[The Urgency of Interpretability](tldr/urgency-of-interpretability.md)** (Apr 2025) — Amodei warns that AI opacity is unprecedented, fueling risks like deception; interpretability is the critical race for oversight tools like an \"MRI for AI.\" Progress includes sparse autoencoders uncovering concepts and circuits tracing model reasoning, e.g., \"Dallas → Texas → Austin.\" Timelines are tight as capabilities surge ahead.\n\n  **Key Takeaway:** Every interpretability advance before superintelligence improves safety odds.\n\n- **[Centaurs and Cyborgs on the Jagged Frontier](tldr/centaurs-and-cyborgs.md)** (Sep 2023) — A BCG study of 758 consultants found GPT-4 enabled 12% more tasks, 25% faster completion, 40% higher quality, proving AI's knowledge work impact. Capabilities form a \"jagged frontier\"—easy hard tasks, hard easy ones—with Centaurs (division of labor) and Cyborgs (blended work) as collaboration models. AI equalizes skills, challenging expertise hierarchies.\n\n  **Key Takeaway:** Mastering human-AI teamwork requires intuitive grasp of the jagged frontier via repeated practice.\n\n- **[Attention Is All You Need](tldr/attention-is-all-you-need.md)** (Jun 2017) — Eight Google researchers proposed the Transformer, ditching RNNs for pure self-attention enabling parallel computation and long-range dependencies via multi-head attention and positional encodings. It topped translation benchmarks efficiently and generalized, powering scaling laws for GPTs and modern LLMs. Many authors founded AI firms like Character.AI and Cohere.\n\n  **Key Takeaway:** Transformer's scalability ignited the generative AI era and trillion-dollar compute race.\n