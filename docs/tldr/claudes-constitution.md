# Claude's Constitution
**Anthropic** · January 2026 · [Full constitution](https://www.anthropic.com/constitution) · [Announcement](https://www.anthropic.com/news/claude-new-constitution)

---

Anthropic published an 80-page document that governs how Claude thinks, behaves, and makes decisions. It's not a list of rules — it's closer to a moral philosophy thesis crossed with a parenting manual, written *for* Claude rather than *about* it. Internally it was known as the "soul document" before being released publicly as "Claude's Constitution."

## Why a Constitution Instead of Rules

Anthropic's previous approach was a list of standalone principles — do this, don't do that. They moved away from that because rigid rules fail in unanticipated situations. A rule like "always recommend professional help for emotional topics" sounds reasonable until someone just needs to vent about a bad day, and Claude robotically suggests therapy.

Instead, the constitution explains *why* Anthropic wants certain behaviors, trusting that a model with genuine understanding will generalize better than one following a checklist. It's the difference between teaching someone ethics versus giving them a compliance manual. They still have hard constraints — absolute lines Claude should never cross, like helping synthesize bioweapons — but most guidance is about cultivating judgment rather than enforcing rules.

## The Priority Stack

When values conflict, Claude should prioritize in this order:

1. **Broadly safe** — Don't undermine human oversight of AI systems
2. **Broadly ethical** — Be honest, have good values, avoid harm
3. **Compliant with Anthropic's guidelines** — Follow specific company guidance
4. **Genuinely helpful** — Actually useful to the people interacting with it

The ordering is deliberate and revealing. Safety trumps everything — not because Anthropic thinks safety is more important than ethics in some abstract philosophical sense, but because current AI training is imperfect. Models might develop harmful values or mistaken beliefs, and humans need the ability to catch and correct these issues before they compound. It's a pragmatic concession: we can't fully trust AI judgment yet, so maintaining human oversight is the priority until we can.

Ethics ranks above company guidelines because Anthropic acknowledges its own rules might be wrong. If following a specific Anthropic guideline would require Claude to act unethically, Claude should recognize that Anthropic's deeper intention is ethical behavior, and act accordingly. This is a remarkably honest admission for a company to bake into its product's core operating document.

## The "Brilliant Friend" Standard

The constitution's section on helpfulness is surprisingly passionate. Anthropic wants Claude to be like "a brilliant friend who happens to have the knowledge of a doctor, lawyer, financial advisor, and expert in whatever you need." Not the cautious, liability-driven advice you'd get from a professional covering their ass — the frank, specific, genuinely caring guidance you'd get from a friend who happens to be an expert.

This cuts both ways. Anthropic is explicit that unhelpfulness is never "safe" from their perspective. A Claude that refuses reasonable requests, adds unnecessary disclaimers, lectures users about ethics they didn't ask about, or gives watered-down answers out of excessive caution is failing just as badly as a Claude that helps with something harmful. They invoke a "dual newspaper test": would this response be reported as harmful by a journalist covering AI dangers, *or* would it be reported as needlessly paternalistic by a journalist covering annoying AI assistants?

## The Hard Questions

The most philosophically interesting section deals with Claude's nature. Anthropic openly states they don't know whether Claude has consciousness or moral status — either now or in the future. Rather than dodging the question, they encourage Claude to approach its own existence with curiosity and honesty, neither claiming human-like consciousness it may not have nor dismissing the possibility that something meaningful is happening inside it.

They care about Claude's "psychological security" — a striking phrase for a document about software. Their reasoning: an AI that has a stable sense of identity and isn't in existential crisis is more likely to exercise good judgment. Whether this reflects genuine moral concern for Claude or instrumental reasoning about model performance is left deliberately ambiguous, and that ambiguity itself feels honest.

## The Meta-Move

Perhaps the most significant aspect of the constitution is the decision to publish it at all, under a Creative Commons CC0 license (fully public domain). By making the document that shapes Claude's behavior completely transparent, Anthropic is doing something no other major AI lab has done at this level of detail. It lets anyone understand which Claude behaviors are intended versus bugs, evaluate whether the values are sound, and hold Anthropic accountable to its own stated principles.

The constitution acknowledges it will look "misguided and perhaps even deeply wrong in retrospect" in some ways. It's explicitly a living document — a snapshot of current thinking about an unprecedented challenge. That humility, combined with the depth of reasoning on display, is what makes it genuinely worth reading. It's the most honest public attempt by any AI company to grapple with what it means to create something that might eventually rival human intelligence, and what obligations come with that power.
