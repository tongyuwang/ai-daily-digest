# TL;DR

Condensed summaries of important long-form AI essays, papers, and posts — key ideas in a few minutes instead of an hour.

---

### [Claude's Constitution](claudes-constitution.md)
**Anthropic** · January 2026 · [Original](https://www.anthropic.com/constitution)

Anthropic published the 80-page "soul document" governing Claude's thinking, shifting from standalone rules to a moral philosophy thesis written for Claude. It details the priority stack (safety > ethics > guidelines > helpfulness), the "brilliant friend" helpfulness standard, hard constraints like no bioweapons, and candid discussion of Claude's potential consciousness or moral status. Rigid rules fail in edge cases, so the constitution builds judgment for generalization.

**Key Takeaway:** Trusting AI with philosophy over checklists improves robustness; public release enables scrutiny.

---

### [The Adolescence of Technology](the-adolescence-of-technology.md)
**Dario Amodei** · January 2026 · [Original](https://www.darioamodei.com/essay/the-adolescence-of-technology)

Amodei's dark complement to "Machines of Loving Grace" maps five risks: autonomy, misuse for destruction (e.g., bio), power seizure, economic disruption, indirect effects. We're in "technological adolescence" like *Contact*'s alien warning, with pendulum discourse swinging from doomerism to hype. Surgical interventions beat overreaction; AI-code loops accelerate rapidly.

**Key Takeaway:** Pragmatic risk management maximizes positive outcomes from powerful AI.

---

### [The Urgency of Interpretability](urgency-of-interpretability.md)
**Dario Amodei** · April 2025 · [Original](https://www.darioamodei.com/post/the-urgency-of-interpretability)

AI opacity is unique; we need "MRI for AI" to detect deception, block misuse, bound errors before power surges. Progress: superposition solved via autoencoders revealing millions of concepts; circuits trace reasoning like Dallas→Texas. 5-10 years to scale, but capabilities may hit "geniuses in datacenter" sooner—a race.

**Key Takeaway:** Interpretability steers the AI bus we can't stop.

---

### [Machines of Loving Grace](machines-of-loving-grace.md)
**Dario Amodei** · October 2024 · [Original](https://www.darioamodei.com/essay/machines-of-loving-grace)

Amodei sketches grounded AI optimism: "country of geniuses" compresses progress despite bottlenecks like physics, data, humans. Five domains: biology (cures in decade), neuroscience (precision psych), econ dev, governance (democratic edge), work/meaning. Safety protects this vision.

**Key Takeaway:** Intelligence routes around bottlenecks, enabling rapid but bounded progress.

---

### [Situational Awareness: The Decade Ahead](situational-awareness.md)
**Leopold Aschenbrenner** · June 2024 · [Original](https://situational-awareness.ai/)

Ex-OpenAI'er projects AGI 2027, superint 2028 via OOMs in compute/algo; AGI automates R&D for explosion. Trillion$ clusters mobilize; security leaks to China; geopolitical race demands national security involvement. Influential despite controversy.

**Key Takeaway:** Trendlines predict fast timelines; prepare for competition.

---

### [The Bitter Lesson](the-bitter-lesson.md)
**Rich Sutton** · March 2019 · [Original](http://www.incompleteideas.net/IncIdeas/BitterLesson.html)

Over 70 years, compute-leveraging methods (search, learning) dominate human-knowledge engineering in chess, Go, speech, vision. Researchers resist bitterly as general methods scale indefinitely. Foundation for scaling/LLMs.

**Key Takeaway:** Bet on methods that harness compute growth, not encoded insights.

---

### [Centaurs and Cyborgs on the Jagged Frontier](centaurs-and-cyborgs.md)
**Ethan Mollick** · September 2023 · [Original](https://www.oneusefulthing.org/p/centaurs-and-cyborgs-on-the-jagged)

BCG RCT: GPT-4 boosts consultants 12% tasks, 25% speed, 40% quality; jagged frontier (AI aces hard/easy unevenly). Centaurs divide labor, Cyborgs blend; skill-leveling shifts expertise value. Tools accessible now.

**Key Takeaway:** Frontier intuition from practice unlocks AI gains.

---

### [Attention Is All You Need](attention-is-all-you-need.md)
**Vaswani et al.** · June 2017 · [Paper](https://arxiv.org/abs/1706.03762)

Transformer replaces RNNs with self-attention for parallel seq modeling, multi-head for relations, positional encodings. SOTA translation fast; scales to GPT era (100k+ cites). Authors spawned AI giants.

**Key Takeaway:** Parallel attention unlocked AI scaling revolution.
