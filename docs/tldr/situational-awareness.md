# Situational Awareness: The Decade Ahead
**Leopold Aschenbrenner** · June 2024 · [Original](https://situational-awareness.ai/)

---

"You can see the future first in San Francisco." So opens the most controversial and widely-discussed AI document of 2024 — a 165-page essay series written by a former OpenAI researcher who argues, with detailed technical reasoning, that AGI will arrive by 2027 and superintelligence within a year after that.

Leopold Aschenbrenner was 22 when he published this. He'd been fired from OpenAI (reportedly for raising security concerns internally), and the essay reads like someone who decided that if the institutions wouldn't listen, the public should know. It went on to help him raise $1.5 billion for his new AI company, backed by Stripe's Collison brothers.

## The Core Argument

Aschenbrenner's method is straightforward: count the "OOMs" — orders of magnitude of improvement. GPT-2 to GPT-4 represented roughly a preschooler-to-smart-high-schooler jump in about four years. That improvement came from three sources: more compute (~0.5 OOMs/year), better algorithms (~0.5 OOMs/year), and "unhobbling" gains (turning chatbots into agents). Trace those trendlines forward, and by 2027 you get another jump of equal magnitude — from smart high schooler to something beyond the best human experts.

The argument is disarmingly simple, which is what makes it so hard to dismiss. He's not predicting a single breakthrough or relying on speculative architecture changes. He's just projecting curves that have held steady for a decade, backed by the observable fact that every major AI lab is now planning $100 billion+ compute clusters.

## The Intelligence Explosion

Chapter II is where it gets truly wild. Once you have AGI — AI systems as capable as the best human AI researchers — those systems can automate AI research itself. Hundreds of millions of AGI instances, running 10-100x faster than humans, could compress a decade of algorithmic progress into less than a year. The gap between human-level AI and vastly superhuman AI might be startlingly brief.

This is the piece that most critics push back on. The counterarguments — diminishing returns, physical bottlenecks, data limitations — are real. But Aschenbrenner addresses them directly, and his track record on near-term predictions has been uncomfortably accurate so far.

## The Trillion-Dollar Cluster

The industrial mobilization chapter reads like a defense policy paper. American electricity production growing by tens of percent. Every available power contract locked up through the end of the decade. Trillions of dollars flowing into GPU manufacturing, datacenters, and energy infrastructure. He wrote this in June 2024; by early 2026, much of what he described was already underway, with Microsoft, Google, and others announcing exactly the scale of buildout he predicted.

## The Security Problem

Perhaps the most prescient chapter argues that AI labs are essentially handing their most valuable secrets to China on a silver platter. Lab security, as of mid-2024, was woefully inadequate for protecting what may become the most strategically important technology since nuclear weapons. Aschenbrenner calls for treating model weights and training techniques with the same seriousness as nuclear secrets — and he makes a compelling case that we're nowhere close to doing so.

## The Geopolitical Stakes

The final major chapters frame AI development as fundamentally a great power competition. Superintelligence will confer decisive economic and military advantage. China isn't out of the race. The free world's survival, Aschenbrenner argues, may literally depend on maintaining AI leadership. Eventually, the national security state will get involved — "somewhere in a SCIF, the endgame will be on."

## Why It Matters

Situational Awareness is polarizing by design. Critics call it alarmist, or point out that aggressive timelines have been wrong before. Supporters note that Aschenbrenner has been more right than wrong on near-term predictions, and that dismissing the argument requires explaining why the trendlines will suddenly break.

What's undeniable is its influence. The essay shaped how policymakers, investors, and AI researchers think about timelines. It put specific numbers and reasoning behind what had previously been vague intuitions about "AI moving fast." And it asked a question that nobody has satisfactorily answered: if even a fraction of this is right, why isn't everyone acting like it?

The essay is dedicated to Ilya Sutskever — the OpenAI co-founder who, like Aschenbrenner, seemed to see something coming that others weren't ready to confront.
