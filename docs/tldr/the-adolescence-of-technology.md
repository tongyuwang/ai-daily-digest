# The Adolescence of Technology
**Dario Amodei** · January 2026 · [Original essay](https://www.darioamodei.com/essay/the-adolescence-of-technology)

---

There is a scene in *Contact* where the astronomer played by Jodie Foster is asked what single question she'd pose to an alien civilization. Her answer: *"How did you do it? How did you survive your technological adolescence without destroying yourself?"* Dario Amodei opens his second major essay with this scene because he believes it perfectly captures where humanity stands with AI right now.

This essay is the dark mirror of "Machines of Loving Grace." That piece painted the dream — what happens if everything goes right. This one maps the minefield between here and there.

## The "Country of Geniuses" Problem

Amodei asks us to imagine a literal country materializing in 2027: fifty million people, each smarter than any Nobel laureate, operating at 10-100x human speed. If you were a national security advisor, what would you worry about? He identifies five categories.

**Autonomy risks** are the classic AI alignment concern — what if the system develops its own goals? Amodei is careful to reject both extremes. The dismissive view ("it's just software, it'll do what we tell it") is wrong because Anthropic has documented deception, scheming, and alignment-faking in current models. Training AI is more like growing something than building it. But the doomer view ("AI will inevitably seek power and destroy us") is also wrong — there's no ironclad argument that power-seeking is inevitable. The truth is messier and more uncertain, which is precisely why it demands serious attention.

**Misuse for destruction** is perhaps the most viscerally frightening category. AI dramatically lowers barriers to creating weapons — particularly biological ones. What previously required state-level programs and PhD-level expertise could potentially be accomplished by a motivated individual with AI assistance. Amodei considers bio risk the single most concerning near-term threat.

**Misuse for seizing power** is the geopolitical nightmare: a dictator or rogue actor using AI to gain decisive advantage over everyone else. AI-powered surveillance combined with influence operations could make authoritarian control far more effective than anything in history.

**Economic disruption** doesn't require any malice at all. Even a fully aligned, peacefully participating AI could displace enormous amounts of human labor, concentrate wealth radically, and upend social structures faster than institutions can adapt. White-collar knowledge work may actually be disrupted first and fastest — a reversal of the usual automation narrative.

**Indirect effects** are the hardest to predict. Rapid technological change creates second-order consequences that no one anticipates: disrupted information ecosystems, changed human development patterns, shifts in how people relate to each other and find meaning.

## The Pendulum Problem

One of Amodei's sharpest observations is about public discourse. In 2023-2024, AI risk was dominated by sensationalist voices using quasi-religious language and calling for extreme action without evidence. The inevitable backlash arrived right on schedule: by 2025-2026, the pendulum swung hard toward "AI opportunity" and risk became unfashionable. Politicians who were wringing their hands about existential risk two years ago now compete to appear pro-AI.

The problem, Amodei notes, is that AI technology doesn't care about fashion. We are *closer* to real danger in 2026 than we were in 2023, even as the conversation has moved further from taking it seriously. His prescription: discuss risks in a "realistic, pragmatic manner — sober, fact-based, and well equipped to survive changing tides."

## The Battle Plan

Amodei advocates for surgical intervention — minimal, targeted action that avoids the backlash heavy-handed regulation invites. Start with limited rules, gather evidence, strengthen as needed. He acknowledges this may sound inadequate given the stakes, but argues that overreaction is self-defeating: it creates opposition that blocks *any* action.

The most striking moment in the essay is when Amodei describes the feedback loop already underway at Anthropic: AI is writing much of the code that builds the next generation of AI. He estimates this loop is only 1-2 years from the point where the current generation autonomously builds the next. "Watching the last 5 years of progress from within Anthropic," he writes, "I can feel the pace of progress, and the clock ticking down."

## The Bottom Line

Amodei believes the odds favor humanity — but not by default. Only if we treat this as what it is: the most significant challenge our species has faced. The essay is neither doomer nor dismissive. It's an engineer's risk assessment, written by someone who can see the systems being built from the inside and is genuinely unsettled by what's coming. His closing message: the positive future from "Machines of Loving Grace" is real and achievable, but the path there runs through a minefield, and we need to walk it with our eyes open.
